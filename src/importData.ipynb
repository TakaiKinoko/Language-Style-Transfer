{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentencesDataset(Dataset):\n",
    "    def __init__(self, text_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            text_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the data\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.   \n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        with open(text_file, \"r\") as f:\n",
    "            self.sentences = f.readlines()\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        sample = self.sentences[idx]\n",
    "            \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateWordEmbeddings(object):\n",
    "    \"\"\"Transform sentence into list of word embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, dim, glove_path):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            dim (int): dimension of word vectors (50, 100, 200, 300)\n",
    "        \"\"\"\n",
    "        \n",
    "        filenames = {\n",
    "            50: \"glove.6B.50d.txt\",\n",
    "            100: \"glove.6B.100d.txt\",\n",
    "            200: \"glove.6B.200d.txt\",\n",
    "            300: \"glove.6B.300d.txt\",\n",
    "        }\n",
    "            \n",
    "        self.embeddings_dict = {}\n",
    "        with open(glove_path+filenames[dim], 'r', encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], \"float32\")\n",
    "                self.embeddings_dict[word] = vector\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            sample (string): sentence to be converted into list of word embeddings\n",
    "        \"\"\"\n",
    "        token_list = self.__tokenize(sample)\n",
    "\n",
    "        sentence_embedding = []\n",
    "        for word in token_list:\n",
    "            try:\n",
    "                vector = self.embeddings_dict[word]\n",
    "                sentence_embedding.append(vector)\n",
    "            except:\n",
    "                print(\"Word \" + word + \" not in GloVe dataset\")\n",
    "                \n",
    "        return sentence_embedding\n",
    "            \n",
    "    def __tokenize(self, string):        \n",
    "        tokens = string.split(sep=\" \")\n",
    "        punctuation = [\" \", \".\", \",\", \".\\n\"]\n",
    "        \n",
    "        tokens_clean = []\n",
    "        for t in tokens:\n",
    "            if t not in punctuation:\n",
    "                tokens_clean.append(t)\n",
    "        \n",
    "        return tokens_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i was sadly mistaken .\n",
      "\n",
      "['i', 'was', 'sadly', 'mistaken']\n",
      "[array([ 1.1891e-01,  1.5255e-01, -8.2073e-02, -7.4144e-01,  7.5917e-01,\n",
      "       -4.8328e-01, -3.1009e-01,  5.1476e-01, -9.8708e-01,  6.1757e-04,\n",
      "       -1.5043e-01,  8.3770e-01, -1.0797e+00, -5.1460e-01,  1.3188e+00,\n",
      "        6.2007e-01,  1.3779e-01,  4.7108e-01, -7.2874e-02, -7.2675e-01,\n",
      "       -7.4116e-01,  7.5263e-01,  8.8180e-01,  2.9561e-01,  1.3548e+00,\n",
      "       -2.5701e+00, -1.3523e+00,  4.5880e-01,  1.0068e+00, -1.1856e+00,\n",
      "        3.4737e+00,  7.7898e-01, -7.2929e-01,  2.5102e-01, -2.6156e-01,\n",
      "       -3.4684e-01,  5.5841e-01,  7.5098e-01,  4.9830e-01, -2.6823e-01,\n",
      "       -2.7443e-03, -1.8298e-02, -2.8096e-01,  5.5318e-01,  3.7706e-02,\n",
      "        1.8555e-01, -1.5025e-01, -5.7512e-01, -2.6671e-01,  9.2121e-01],\n",
      "      dtype=float32), array([ 0.086888, -0.19416 , -0.24267 , -0.33391 ,  0.56731 ,  0.39783 ,\n",
      "       -0.97809 ,  0.03159 , -0.61469 , -0.31406 ,  0.56145 ,  0.12886 ,\n",
      "       -0.84193 , -0.46992 ,  0.47097 ,  0.023012, -0.59609 ,  0.22291 ,\n",
      "       -1.1614  ,  0.3865  ,  0.067412,  0.44883 ,  0.17394 , -0.53574 ,\n",
      "        0.17909 , -2.1647  , -0.12827 ,  0.29036 , -0.15061 ,  0.35242 ,\n",
      "        3.124   , -0.90085 , -0.02567 , -0.41709 ,  0.40565 , -0.22703 ,\n",
      "        0.76829 ,  0.60982 ,  0.070068, -0.13271 , -0.1201  ,  0.096132,\n",
      "       -0.43998 , -0.48531 , -0.5188  , -0.3077  , -0.75028 , -0.77    ,\n",
      "        0.3945  , -0.16937 ], dtype=float32), array([ 0.28444  , -0.39081  , -0.34227  , -0.63226  ,  0.61704  ,\n",
      "       -0.089242 ,  0.0064506,  0.3305   , -0.33915  , -0.031942 ,\n",
      "       -0.12324  ,  0.084546 , -0.45722  , -0.23595  ,  0.69046  ,\n",
      "        0.31212  , -0.25686  ,  0.14431  ,  0.38031  , -0.023955 ,\n",
      "       -0.83765  ,  0.77638  ,  0.21921  ,  0.018424 ,  1.2717   ,\n",
      "       -0.62687  , -0.94035  ,  0.85197  ,  0.65682  , -0.14562  ,\n",
      "        0.73926  ,  0.15616  ,  0.66791  , -0.50493  , -0.27653  ,\n",
      "        0.021019 ,  0.15281  ,  0.27123  , -0.30605  , -0.24318  ,\n",
      "       -0.51949  , -0.3205   , -0.16864  ,  0.33524  , -0.15086  ,\n",
      "        0.27557  , -0.099136 ,  0.15923  ,  0.051963 ,  0.1058   ],\n",
      "      dtype=float32), array([ 0.5921   , -0.7674   , -0.137    , -0.41915  ,  0.83513  ,\n",
      "        0.59569  , -0.060699 ,  0.24445  , -0.44412  ,  0.1724   ,\n",
      "        0.38852  ,  0.083484 , -0.090247 , -0.21001  ,  0.26548  ,\n",
      "        0.093007 , -0.3922   , -0.16617  , -0.1467   ,  0.0075825,\n",
      "       -0.69751  ,  0.62999  ,  0.14182  ,  0.18453  ,  0.77849  ,\n",
      "       -1.3033   , -0.88905  ,  0.44484  ,  1.0111   , -0.18446  ,\n",
      "        0.6578   , -0.48989  , -0.058898 , -0.90253  ,  0.33104  ,\n",
      "       -0.18141  ,  0.22079  , -0.52241  , -0.53233  ,  0.1169   ,\n",
      "       -0.019604 , -0.27922  ,  0.51481  ,  0.26945  ,  0.55688  ,\n",
      "       -0.35066  , -0.2941   ,  0.85832  ,  1.047    , -0.083584 ],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "rawSentences = sentencesDataset(\"data/yelp/sentiment.train.0\", \"data/yelp/\")\n",
    "print(rawSentences[0])\n",
    "\n",
    "yelpData = sentencesDataset(\"data/yelp/sentiment.train.0\", \"data/yelp/\", \n",
    "                            transform=GenerateWordEmbeddings(50, \"data/glove.6B/\"))\n",
    "print(yelpData[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
